{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  298980\n",
      "Test size:  298981\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from matplotlib import cm, colors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import dataUtils as du\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def GetData(filename, train_frac=0.5, branches=['pileup', 'amp', 'phase', 'trace'],  treename=\"OutputTree\"):\n",
    "    '''\n",
    "    Returns TFile as a pandas dataframe\n",
    "    '''\n",
    "    try: \n",
    "        import uproot\n",
    "        import awkward as ak\n",
    "        import numpy as np\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"Please install uproot, awkward, numpy, and pandas\")\n",
    "        return\n",
    "  \n",
    "    file = uproot.open(filename)\n",
    "    tree = file[treename]\n",
    "    data = {}\n",
    "    for branch in branches:\n",
    "        data[branch] = ak.to_numpy(tree[branch].arrays()[branch])\n",
    "    \n",
    "    train_size = int(len(data[\"trace\"])*train_frac)\n",
    "    test_size = len(data[\"trace\"]) - train_size\n",
    "    print(\"Train size: \",train_size)\n",
    "    print(\"Test size: \",test_size)\n",
    "\n",
    "    x = np.array(data[\"trace\"])\n",
    "    y_pileup = np.array(data[\"pileup\"])\n",
    "    y_amp = np.array(data[\"amp\"])\n",
    "    y_phase = np.array(data[\"phase\"])\n",
    "\n",
    "    x_train, x_test, y_pileup_train, y_pileup_test, y_amp_train, y_amp_test, y_phase_train, y_phase_test = train_test_split(x, y_pileup, y_amp, y_phase, test_size=test_size)\n",
    "\n",
    "    return ((x_train, y_pileup_train, y_amp_train, y_phase_train), (x_test, y_pileup_test, y_amp_test, y_phase_test))\n",
    "\n",
    "(x_train, y_pileup_train, y_amp_train, y_phase_train), (x_test, y_pileup_test, y_amp_test, y_phase_test) = GetData(\"../data/Data10percFloat.root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size:  218\n"
     ]
    }
   ],
   "source": [
    "AUGMENTATION = 8\n",
    "TRACELENGTH = 250\n",
    "INPUTSIZE = TRACELENGTH-4*AUGMENTATION\n",
    "print(\"Input size: \", INPUTSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "AUGMENTATION = 8\n",
    "TRACELENGTH = 250\n",
    "INPUTSIZE = TRACELENGTH-4*AUGMENTATION\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "TRAIN_PERCENT = 0.1\n",
    "DATAFILE = \"/home/tmengel/ML4PileUp/data/Data10percFloat.root\"\n",
    "TREENAME = \"OutputTree\"\n",
    "OUTPUT_DIR = \"/home/tmengel/ML4PileUp/network-pruning/\"\n",
    "EPOCHS_SEARCH = 10\n",
    "EPOCHS_FINAL = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "file = \"hyper_config.json\"\n",
    "\n",
    "ModelKeys = {  \n",
    "    \"AUGMENTATION\" : AUGMENTATION,\n",
    "    \"TRACELENGTH\" : TRACELENGTH,\n",
    "    \"INPUTSIZE\" : INPUTSIZE,\n",
    "    \"VALIDATION_SPLIT\" : VALIDATION_SPLIT,\n",
    "    \"TRAIN_PERCENT\" : TRAIN_PERCENT,\n",
    "    \"DATAFILE\" : DATAFILE,\n",
    "    \"TREENAME\" : TREENAME,\n",
    "    \"EPOCHS_SEARCH\" : EPOCHS_SEARCH,\n",
    "    \"EPOCHS_FINAL\" : EPOCHS_FINAL,\n",
    "    \"BATCH_SIZE\" : BATCH_SIZE,\n",
    "    \"DATA_FILE\" : DATAFILE,\n",
    "    \"OUTPUT_DIR\" : OUTPUT_DIR,\n",
    "    \"MODEL_TYPES\" : [\"Amp\", \"Pileup\", \"Phase\"],\n",
    "        \"Amp\": {\n",
    "                \"LOSS\" : \"mse\",\n",
    "                \"OUTPUT_ACVTIVATIONS\" : ['relu', 'linear'],\n",
    "                \"OBJECTIVE\" : \"val_loss\",\n",
    "                \"EARLY_STOPPING_VAR\" : \"val_loss\",\n",
    "                \"EARLY_STOPPING_MODE\" : \"min\",\n",
    "                \"DIRECTORY\" : \"AmpDir\",\n",
    "                \"PROJECT_NAME\" : \"AmpHyperTune\",\n",
    "                \"TUNED_MODEL\" : \"AmpHyperTunedModel\"\n",
    "                },\n",
    "        \"Pileup\" : {\n",
    "                \"LOSS\" : \"bce\",\n",
    "                \"OUTPUT_ACVTIVATIONS\" : ['sigmoid', 'tanh'],\n",
    "                \"OBJECTIVE\" : \"val_accuracy\",\n",
    "                \"EARLY_STOPPING_VAR\" : \"val_accuracy\",\n",
    "                \"EARLY_STOPPING_MODE\" : \"max\",\n",
    "                \"DIRECTORY\" : \"PileupDir\",\n",
    "                \"PROJECT_NAME\" : \"PileupHyperTune\",\n",
    "                \"TUNED_MODEL\" : \"PileupHyperTunedModel\"\n",
    "                },\n",
    "        \"Phase\" : {\n",
    "                \"LOSS\" : \"mse\",\n",
    "                \"OUTPUT_ACVTIVATIONS\" : ['relu', 'linear'],\n",
    "                \"OBJECTIVE\" : \"val_loss\",\n",
    "                \"EARLY_STOPPING_VAR\" : \"val_loss\",\n",
    "                \"EARLY_STOPPING_MODE\" : \"min\",\n",
    "                \"DIRECTORY\" : \"PhaseDir\",\n",
    "                \"PROJECT_NAME\" : \"PhaseHyperTune\",\n",
    "                \"TUNED_MODEL\" : \"PhaseHyperTunedModel\"\n",
    "                }\n",
    "}\n",
    "\n",
    "with open(file, \"w\") as f:\n",
    "    json.dump(ModelKeys, f, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from AmpDir/AmpHyperTune/tuner0.json\n",
      "Train size:  59796\n",
      "Test size:  538165\n",
      "Best hyperparameters:\n",
      "kernel_size_0 9\n",
      "filters_0 32\n",
      "activation_0 tanh\n",
      "max_pooling True\n",
      "pool_size 6\n",
      "dropout_conv True\n",
      "dropout_rate 0.4\n",
      "units_dense0 352\n",
      "activation_dense0 tanh\n",
      "activation_output relu\n",
      "learning_rate 0.0021299066766227647\n",
      "tuner/epochs 2\n",
      "tuner/initial_epoch 0\n",
      "tuner/bracket 0\n",
      "tuner/round 0\n",
      "Epoch 1/10\n",
      "1495/1495 [==============================] - 20s 13ms/step - loss: 0.6158 - accuracy: 0.4770 - val_loss: 0.5994 - val_accuracy: 0.4848\n",
      "Epoch 2/10\n",
      "1495/1495 [==============================] - 19s 13ms/step - loss: 0.6149 - accuracy: 0.4777 - val_loss: 0.5994 - val_accuracy: 0.4848\n",
      "Epoch 3/10\n",
      "1495/1495 [==============================] - 16s 11ms/step - loss: 0.6149 - accuracy: 0.4777 - val_loss: 0.5994 - val_accuracy: 0.4848\n",
      "Epoch 4/10\n",
      "1495/1495 [==============================] - 20s 14ms/step - loss: 0.6149 - accuracy: 0.4777 - val_loss: 0.5994 - val_accuracy: 0.4848\n",
      "Epoch 5/10\n",
      "1495/1495 [==============================] - 22s 15ms/step - loss: 0.6149 - accuracy: 0.4777 - val_loss: 0.5994 - val_accuracy: 0.4848\n",
      "Epoch 6/10\n",
      "1495/1495 [==============================] - 23s 16ms/step - loss: 0.6149 - accuracy: 0.4777 - val_loss: 0.5994 - val_accuracy: 0.4848\n",
      "Best epoch: 1\n",
      "1495/1495 [==============================] - 21s 13ms/step - loss: 0.6160 - accuracy: 0.4770 - val_loss: 0.5994 - val_accuracy: 0.4848\n",
      "16818/16818 [==============================] - 91s 5ms/step - loss: 0.6123 - accuracy: 0.4757\n",
      "[test loss, test accuracy]: [0.6123239994049072, 0.47569796442985535]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "\n",
    "ModelKeys = {  \n",
    "      \"Amp\": {\n",
    "            \"LOSS\" : \"mse\",\n",
    "            \"OUTPUT_ACVTIVATIONS\" : ['relu', 'linear'],\n",
    "            \"OBJECTIVE\" : \"val_loss\",\n",
    "            \"EARLY_STOPPING_VAR\" : \"val_loss\",\n",
    "            \"EARLY_STOPPING_MODE\" : \"min\",\n",
    "            \"DIRECTORY\" : \"AmpDir\",\n",
    "            \"PROJECT_NAME\" : \"AmpHyperTune\",\n",
    "            \"TUNED_MODEL\" : \"AmpHyperTunedModel\"\n",
    "            },\n",
    "    \"Pileup\" : {\n",
    "            \"LOSS\" : \"bce\",\n",
    "            \"OUTPUT_ACVTIVATIONS\" : ['sigmoid', 'tanh'],\n",
    "            \"OBJECTIVE\" : \"val_accuracy\",\n",
    "            \"EARLY_STOPPING_VAR\" : \"val_accuracy\",\n",
    "            \"EARLY_STOPPING_MODE\" : \"max\",\n",
    "            \"DIRECTORY\" : \"PileupDir\",\n",
    "            \"PROJECT_NAME\" : \"PileupHyperTune\",\n",
    "            \"TUNED_MODEL\" : \"PileupHyperTunedModel\"\n",
    "            },\n",
    "    \"Phase\" : {\n",
    "            \"LOSS\" : \"mse\",\n",
    "            \"OUTPUT_ACVTIVATIONS\" : ['relu', 'linear'],\n",
    "            \"OBJECTIVE\" : \"val_loss\",\n",
    "            \"EARLY_STOPPING_VAR\" : \"val_loss\",\n",
    "            \"EARLY_STOPPING_MODE\" : \"min\",\n",
    "            \"DIRECTORY\" : \"PhaseDir\",\n",
    "            \"PROJECT_NAME\" : \"PhaseHyperTune\",\n",
    "            \"TUNED_MODEL\" : \"PhaseHyperTunedModel\"\n",
    "            }\n",
    "}\n",
    "\n",
    "# constants\n",
    "AUGMENTATION = 8\n",
    "TRACELENGTH = 250\n",
    "INPUTSIZE = TRACELENGTH-4*AUGMENTATION\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "TRAIN_PERCENT = 0.1\n",
    "DATAFILE = \"../data/Data10percFloat.root\"\n",
    "\n",
    "EPOCHS_SEARCH = 10\n",
    "EPOCHS_FINAL = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "\n",
    "# Model specific constants\n",
    "TYPE = \"Amp\"\n",
    "VALID_TYPES = ModelKeys.keys()\n",
    "if TYPE not in VALID_TYPES:\n",
    "    print(\"Invalid model type\")\n",
    "    sys.exit(1)\n",
    "\n",
    "LOSS = ModelKeys[TYPE][\"LOSS\"]\n",
    "\n",
    "# get current directory\n",
    "DIRECTORY = f'{os.getcwd()}/{ModelKeys[TYPE][\"DIRECTORY\"]}'\n",
    "PROJECT_NAME = ModelKeys[TYPE][\"PROJECT_NAME\"]\n",
    "\n",
    "OUTPUT_ACVTIVATIONS = ModelKeys[TYPE][\"OUTPUT_ACVTIVATIONS\"]\n",
    "OBJECTIVE = ModelKeys[TYPE][\"OBJECTIVE\"]\n",
    "TUNED_MODEL = ModelKeys[TYPE][\"TUNED_MODEL\"]\n",
    "\n",
    "EARLY_STOPPING_VAR = ModelKeys[TYPE][\"EARLY_STOPPING_VAR\"]\n",
    "EARLY_STOPPING_MODE = ModelKeys[TYPE][\"EARLY_STOPPING_MODE\"]\n",
    "\n",
    "# print out info\n",
    "print(\"====================================\")\n",
    "print(\"Constants\\n\")\n",
    "print(\"AUGMENTATION: \", AUGMENTATION)\n",
    "print(\"TRACELENGTH: \", TRACELENGTH)\n",
    "print(\"INPUTSIZE: \", INPUTSIZE)\n",
    "print(\"VALIDATION_SPLIT: \", VALIDATION_SPLIT)\n",
    "print(\"TRAIN_PERCENT: \", TRAIN_PERCENT)\n",
    "print(\"DATAFILE: \", DATAFILE)\n",
    "print(\"EPOCHS_SEARCH: \", EPOCHS_SEARCH)\n",
    "print(\"EPOCHS_FINAL: \", EPOCHS_FINAL)\n",
    "print(\"BATCH_SIZE: \", BATCH_SIZE)\n",
    "print(\"====================================\")\n",
    "print(\"Model specific constants\\n\")\n",
    "print(\"TYPE: \", TYPE)\n",
    "print(\"LOSS: \", LOSS)\n",
    "print(\"OUTPUT_ACVTIVATIONS: \", OUTPUT_ACVTIVATIONS)\n",
    "print(\"OBJECTIVE: \", OBJECTIVE)\n",
    "print(\"DIRECTORY: \", DIRECTORY)\n",
    "print(\"PROJECT_NAME: \", PROJECT_NAME)\n",
    "print(\"TUNED_MODEL: \", TUNED_MODEL)\n",
    "print(\"EARLY_STOPPING_VAR: \", EARLY_STOPPING_VAR)\n",
    "print(\"EARLY_STOPPING_MODE: \", EARLY_STOPPING_MODE)\n",
    "print(\"====================================\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def GetData(filename, train_frac=0.5, branches=['pileup', 'amp', 'phase', 'trace'],  treename=\"OutputTree\"):\n",
    "    '''\n",
    "    Returns TFile as a pandas dataframe\n",
    "    '''\n",
    "    try: \n",
    "        import uproot\n",
    "        import awkward as ak\n",
    "        import numpy as np\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"Please install uproot, awkward, numpy, and pandas\")\n",
    "        return\n",
    "  \n",
    "    file = uproot.open(filename)\n",
    "    tree = file[treename]\n",
    "    data = {}\n",
    "    for branch in branches:\n",
    "        data[branch] = ak.to_numpy(tree[branch].arrays()[branch])\n",
    "    \n",
    "    train_size = int(len(data[\"trace\"])*train_frac)\n",
    "    test_size = len(data[\"trace\"]) - train_size\n",
    "    print(\"Train size: \",train_size)\n",
    "    print(\"Test size: \",test_size)\n",
    "\n",
    "    x = np.array(data[\"trace\"])\n",
    "    y_pileup = np.array(data[\"pileup\"])\n",
    "    y_amp = np.array(data[\"amp\"])\n",
    "    y_phase = np.array(data[\"phase\"])\n",
    "\n",
    "    x_train, x_test, y_pileup_train, y_pileup_test, y_amp_train, y_amp_test, y_phase_train, y_phase_test = train_test_split(x, y_pileup, y_amp, y_phase, test_size=test_size)\n",
    "\n",
    "    return ((x_train, y_pileup_train, y_amp_train, y_phase_train), (x_test, y_pileup_test, y_amp_test, y_phase_test))\n",
    "\n",
    "def GetDataForModel(filename, model_type, train_frac=0.5,  treename=\"OutputTree\"):\n",
    "     \n",
    "    # get the data\n",
    "    (x_train, y_pileup_train, y_amp_train, y_phase_train), (x_test, y_pileup_test, y_amp_test, y_phase_test) = GetData(filename, train_frac=train_frac, branches=['pileup', 'amp', 'phase', 'trace'], treename=treename)\n",
    "\n",
    "    # get the model specific data\n",
    "    if model_type == \"Amp\":\n",
    "        y_train = y_amp_train\n",
    "        y_test = y_amp_test\n",
    "    elif model_type == \"Pileup\":\n",
    "        y_train = y_pileup_train\n",
    "        y_test = y_pileup_test\n",
    "    elif model_type == \"Phase\":\n",
    "        y_train = y_phase_train\n",
    "        y_test = y_phase_test\n",
    "    else:\n",
    "        print(\"Invalid model type\")\n",
    "        return None\n",
    "    \n",
    "    return ((x_train, y_train), (x_test, y_test))\n",
    "\n",
    "def ModelHyperTune(hp):\n",
    "        \n",
    "        model = keras.Sequential()\n",
    "\n",
    "        # input layer\n",
    "        kernel_size_input = hp.Int('kernel_size_0', min_value=2, max_value=18, step=4)\n",
    "        filters_input = hp.Int('filters_0', min_value=16, max_value=128, step=16)\n",
    "        activation_input = hp.Choice('activation_0', values=['tanh', 'relu'])\n",
    "\n",
    "        model.add(keras.layers.Conv1D(kernel_size=kernel_size_input, \n",
    "                                      filters=filters_input, \n",
    "                                      activation=activation_input, \n",
    "                                      name='conv0', \n",
    "                                      input_shape=(INPUTSIZE,1)))\n",
    "\n",
    "        # max pooling options\n",
    "        max_pooling = hp.Boolean('max_pooling')\n",
    "        pool_size = hp.Int('pool_size', min_value=2, max_value=8, step=2)\n",
    "        # add max pooling\n",
    "        if max_pooling:\n",
    "                model.add(keras.layers.MaxPooling1D(pool_size=pool_size, name='max_pooling0'))        \n",
    "\n",
    "        # drouput options\n",
    "        dropout_conv = hp.Boolean('dropout_conv')       \n",
    "        dropout_conv_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n",
    "        # add dropout\n",
    "        if dropout_conv:\n",
    "                model.add(keras.layers.Dropout(dropout_conv_rate, name='dropout_conv0'))\n",
    "        \n",
    "        # # hidden conv layers\n",
    "        for i in range(hp.Int('num_conv_layers', min_value=0, max_value=3, step=1)):\n",
    "                kernel_size = hp.Int(f'kernel_size_{i+1}', min_value=1, max_value=16, step=4)\n",
    "                filters = hp.Int(f'filters_{i+1}', min_value=16, max_value=128, step=16)\n",
    "                activation = hp.Choice(f'activation_{i+1}', values=['tanh', 'relu'])\n",
    "\n",
    "                model.add(keras.layers.Conv1D(kernel_size=kernel_size, \n",
    "                                              filters=filters, \n",
    "                                              activation=activation, \n",
    "                                              name=f'conv{i+1}'))\n",
    "                # add max pooling\n",
    "                if max_pooling:\n",
    "                        model.add(keras.layers.MaxPooling1D(pool_size=pool_size, name=f'max_pooling{i+1}'))\n",
    "\n",
    "                # add dropout\n",
    "                if dropout_conv:\n",
    "                        model.add(keras.layers.Dropout(dropout_conv_rate, name=f'dropout_conv{i+1}'))\n",
    "        \n",
    "        # # flatten layer\n",
    "        model.add(keras.layers.Flatten(name='flat0'))\n",
    "\n",
    "        # initial dense layer\n",
    "        units_dense = hp.Int('units_dense0', min_value=32, max_value=512, step=32)\n",
    "        activation_dense = hp.Choice('activation_dense0', values=['tanh', 'relu'])\n",
    "        model.add(keras.layers.Dense(units=units_dense, activation=activation_dense, name='dense0'))\n",
    "\n",
    "        # # hidden dense layers\n",
    "        dropout_dense = hp.Boolean('dropout_dense')\n",
    "        dropout_dense_rate = hp.Float('dropout_rate_dense', min_value=0.1, max_value=0.5, step=0.1)\n",
    "\n",
    "        if dropout_dense:\n",
    "                model.add(keras.layers.Dropout(dropout_dense_rate, name='dropout_dense0'))\n",
    "\n",
    "        # add hidden dense layers\n",
    "        for i in range(hp.Int('num_dense_layers', min_value=0, max_value=3, step=1)):\n",
    "                units = hp.Int(f'units_dense_{i+1}', min_value=32, max_value=512, step=32)\n",
    "                activation = hp.Choice(f'activation_dense_{i+1}', values=['tanh', 'relu'])\n",
    "                model.add(keras.layers.Dense(units=units, activation=activation, name=f'dense{i+1}'))\n",
    "\n",
    "                if dropout_dense:\n",
    "                        model.add(keras.layers.Dropout(dropout_dense_rate, name=f'dropout_dense{i+1}'))\n",
    "                                  \n",
    "        \n",
    "        # # output layer\n",
    "        avtivation_output = hp.Choice('activation_output', values=OUTPUT_ACVTIVATIONS)\n",
    "        model.add(keras.layers.Dense(1, activation=avtivation_output, name='output'))\n",
    "\n",
    "        # compile model\n",
    "        lr = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "                            loss=LOSS,\n",
    "                            metrics=[LOSS, 'accuracy']\n",
    "                            )\n",
    "        \n",
    "        return model\n",
    "\n",
    "def PrintBestHyperparameters(best_hps):\n",
    "    best_hps_dict = best_hps.values\n",
    "    print(\"Best hyperparameters:\")\n",
    "    for key in best_hps_dict:\n",
    "        print(key, best_hps_dict[key])\n",
    "\n",
    "\n",
    "tuner = kt.Hyperband(ModelHyperTune,\n",
    "                     objective=OBJECTIVE,\n",
    "                     max_epochs=EPOCHS_SEARCH,\n",
    "                     overwrite=True,\n",
    "                     factor=3,\n",
    "                     directory=str(DIRECTORY),\n",
    "                     project_name=str(PROJECT_NAME)\n",
    "                     )\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "                                              monitor=EARLY_STOPPING_VAR,\n",
    "                                              patience=5, \n",
    "                                              restore_best_weights=True, \n",
    "                                              min_delta=0.001, \n",
    "                                              mode=EARLY_STOPPING_MODE\n",
    "                                            )\n",
    "\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = GetDataForModel(filename= DATAFILE, \n",
    "                                                       model_type=TYPE, \n",
    "                                                       train_frac=TRAIN_PERCENT)\n",
    "\n",
    "tuner.search(x_train, y_train, epochs=EPOCHS_SEARCH, validation_split=VALIDATION_SPLIT, callbacks=[stop_early])\n",
    "\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "PrintBestHyperparameters(best_hps)\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=EPOCHS_FINAL,\n",
    "                    validation_split=VALIDATION_SPLIT, \n",
    "                    callbacks=[stop_early])\n",
    "\n",
    "val_obj_per_epoch = history.history[OBJECTIVE]\n",
    "best_epoch = val_obj_per_epoch.index(max(val_obj_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it on the data\n",
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# # Retrain the model\n",
    "hypermodel.fit(x_train, y_train,\n",
    "               epochs=best_epoch, \n",
    "               validation_split=VALIDATION_SPLIT, \n",
    "               callbacks=[stop_early])\n",
    "\n",
    "eval_result = hypermodel.evaluate(x_test, y_test)\n",
    "print(\"Eval result: \", eval_result)\n",
    "\n",
    "# Save the model\n",
    "hypermodel.save(f'{DIRECTORY}/{TUNED_MODEL}.h5')\n",
    "# Save the best hyperparameters\n",
    "with open(f'{DIRECTORY}/{TUNED_MODEL}.txt', 'w') as f:\n",
    "    best_hps_dict = best_hps.values\n",
    "    for key in best_hps_dict:\n",
    "        f.write(key + ' ' + str(best_hps_dict[key]) + '\\n')\n",
    "    f.write('Best epoch: %d' % (best_epoch,))\n",
    "    f.write('\\n Eval result: ' + str(eval_result))\n",
    "    f.write('\\n')\n",
    "\n",
    "# Save the history\n",
    "import pickle\n",
    "with open(f'{DIRECTORY}/{TUNED_MODEL}_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "# Save the tuner\n",
    "tuner.results_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel_size_0': 9,\n",
       " 'filters_0': 32,\n",
       " 'activation_0': 'tanh',\n",
       " 'max_pooling': True,\n",
       " 'pool_size': 6,\n",
       " 'dropout_conv': True,\n",
       " 'dropout_rate': 0.4,\n",
       " 'units_dense0': 352,\n",
       " 'activation_dense0': 'tanh',\n",
       " 'activation_output': 'relu',\n",
       " 'learning_rate': 0.0021299066766227647,\n",
       " 'tuner/epochs': 2,\n",
       " 'tuner/initial_epoch': 0,\n",
       " 'tuner/bracket': 0,\n",
       " 'tuner/round': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hps.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(x_train, y_amp_train, epochs=50, validation_split=VALIDATION_SPLIT, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  298980\n",
      "Test size:  298981\n"
     ]
    }
   ],
   "source": [
    "def GetData(filename, train_frac=0.5, branches=['pileup', 'amp', 'phase', 'trace'],  treename=\"OutputTree\"):\n",
    "    '''\n",
    "    Returns TFile as a pandas dataframe\n",
    "    '''\n",
    "    try: \n",
    "        import uproot\n",
    "        import awkward as ak\n",
    "        import numpy as np\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"Please install uproot, awkward, numpy, and pandas\")\n",
    "        return\n",
    "  \n",
    "    file = uproot.open(filename)\n",
    "    tree = file[treename]\n",
    "    data = {}\n",
    "    for branch in branches:\n",
    "        data[branch] = ak.to_numpy(tree[branch].arrays()[branch])\n",
    "    \n",
    "    train_size = int(len(data[\"trace\"])*train_frac)\n",
    "    test_size = len(data[\"trace\"]) - train_size\n",
    "    print(\"Train size: \",train_size)\n",
    "    print(\"Test size: \",test_size)\n",
    "\n",
    "    x = np.array(data[\"trace\"])\n",
    "    y_pileup = np.array(data[\"pileup\"])\n",
    "    y_amp = np.array(data[\"amp\"])\n",
    "    y_phase = np.array(data[\"phase\"])\n",
    "\n",
    "    x_train, x_test, y_pileup_train, y_pileup_test, y_amp_train, y_amp_test, y_phase_train, y_phase_test = train_test_split(x, y_pileup, y_amp, y_phase, test_size=test_size)\n",
    "\n",
    "    return ((x_train, y_pileup_train, y_amp_train, y_phase_train), (x_test, y_pileup_test, y_amp_test, y_phase_test))\n",
    "\n",
    "(x_train, y_pileup_train, y_amp_train, y_phase_train), (x_test, y_pileup_test, y_amp_test, y_phase_test) = GetData(\"../data/Data10percFloat.root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def PhaseNet(inputsize):\n",
    "    model = keras.Sequential([\n",
    "        layers.Conv1D(kernel_size=10, filters=64, activation='tanh', name='conv1', input_shape=(inputsize,1)),\n",
    "        layers.Dropout(0.2, name='dropout'),\n",
    "        layers.Conv1D(kernel_size=1, filters=64, activation='relu', name='conv2'),\n",
    "        layers.Dropout(0.2, name='dropout'),\n",
    "        layers.Conv1D(kernel_size=2, filters=64, activation='relu', padding='same', name='conv3'),\n",
    "        layers.Dropout(0.2, name='dropout'),\n",
    "        layers.Flatten(name='flat1'),\n",
    "        layers.Dense(128, activation='relu', name='dense1'),\n",
    "        layers.Dense(1, activation='linear', name='output')\n",
    "    ],\n",
    "    name='PhaseNet')\n",
    "\n",
    "    return model\n",
    "\n",
    "NUMEPOCHS = 50\n",
    "BATCHSIZE = 32\n",
    "PERCENTPILEUP = 0.5\n",
    "ModelOutputName = 'tanner_test'\n",
    "INPUTSIZE = TRACELENGTH-4*AUGMENTATION\n",
    "\n",
    "model = PhaseNet(INPUTSIZE)\n",
    "\n",
    "# early stopping callback based on validation loss\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=10,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                loss='mean_squared_error',\n",
    "                metrics=['mean_squared_error'], \n",
    "                callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = du.GetDataSet(type=\"phase\",\n",
    "                                                 fname=\"DataSmallFloat.root\",\n",
    "                                                 tname=\"OutputTree\")\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, epochs=NUMEPOCHS, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 and 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Define the model architecture.\n",
    "model = keras.Sequential([\n",
    "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  epochs=4,\n",
    "  validation_split=0.1,\n",
    ")\n",
    "\n",
    "_, baseline_model_accuracy = model.evaluate(\n",
    "    test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "\n",
    "_, keras_file = tempfile.mkstemp('.h5')\n",
    "tf.keras.models.save_model(model, keras_file, include_optimizer=False)\n",
    "print('Saved baseline model to:', keras_file)\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Compute end step to finish pruning after 2 epochs.\n",
    "batch_size = 128\n",
    "epochs = 2\n",
    "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
    "\n",
    "num_images = train_images.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                               final_sparsity=0.80,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_for_pruning.summary()\n",
    "\n",
    "logdir = tempfile.mkdtemp()\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "]\n",
    "\n",
    "model_for_pruning.fit(train_images, train_labels,\n",
    "                  batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "\n",
    "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
    "   test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy) \n",
    "print('Pruned test accuracy:', model_for_pruning_accuracy)\n",
    "\n",
    "\n",
    "#docs_infra: no_execute\n",
    "%tensorboard --logdir={logdir}\n",
    "\n",
    "\n",
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
    "tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
    "print('Saved pruned Keras model to:', pruned_keras_file)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "pruned_tflite_model = converter.convert()\n",
    "\n",
    "_, pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(pruned_tflite_file, 'wb') as f:\n",
    "  f.write(pruned_tflite_model)\n",
    "\n",
    "print('Saved pruned TFLite model to:', pruned_tflite_file)\n",
    "\n",
    "def get_gzipped_model_size(file):\n",
    "  # Returns size of gzipped model, in bytes.\n",
    "  import os\n",
    "  import zipfile\n",
    "\n",
    "  _, zipped_file = tempfile.mkstemp('.zip')\n",
    "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(file)\n",
    "\n",
    "  return os.path.getsize(zipped_file)\n",
    "\n",
    "\n",
    "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
    "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
    "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))\n",
    "\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_and_pruned_tflite_model = converter.convert()\n",
    "\n",
    "_, quantized_and_pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(quantized_and_pruned_tflite_file, 'wb') as f:\n",
    "  f.write(quantized_and_pruned_tflite_model)\n",
    "\n",
    "print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)\n",
    "\n",
    "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
    "print(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(interpreter):\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on ever y image in the \"test\" dataset.\n",
    "  prediction_digits = []\n",
    "  for i, test_image in enumerate(test_images):\n",
    "    if i % 1000 == 0:\n",
    "      print('Evaluated on {n} results so far.'.format(n=i))\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    digit = np.argmax(output()[0])\n",
    "    prediction_digits.append(digit)\n",
    "\n",
    "  print('\\n')\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  prediction_digits = np.array(prediction_digits)\n",
    "  accuracy = (prediction_digits == test_labels).mean()\n",
    "  return accuracy\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=quantized_and_pruned_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy = evaluate_model(interpreter)\n",
    "\n",
    "print('Pruned and quantized TFLite test_accuracy:', test_accuracy)\n",
    "print('Pruned TF test accuracy:', model_for_pruning_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Load MNIST dataset.\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 and 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "pruning_params_2_by_4 = {\n",
    "    'sparsity_m_by_n': (2, 4),\n",
    "}\n",
    "\n",
    "pruning_params_sparsity_0_5 = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(target_sparsity=0.5,\n",
    "                                                              begin_step=0,\n",
    "                                                              frequency=100)\n",
    "}\n",
    "\n",
    "model = keras.Sequential([\n",
    "    prune_low_magnitude(\n",
    "        keras.layers.Conv2D(\n",
    "            32, 5, padding='same', activation='relu',\n",
    "            input_shape=(28, 28, 1),\n",
    "            name=\"pruning_sparsity_0_5\"),\n",
    "        **pruning_params_sparsity_0_5),\n",
    "    keras.layers.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "    prune_low_magnitude(\n",
    "        keras.layers.Conv2D(\n",
    "            64, 5, padding='same',\n",
    "            name=\"structural_pruning\"),\n",
    "        **pruning_params_2_by_4),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.ReLU(),\n",
    "    keras.layers.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "    keras.layers.Flatten(),\n",
    "    prune_low_magnitude(\n",
    "        keras.layers.Dense(\n",
    "            1024, activation='relu',\n",
    "            name=\"structural_pruning_dense\"),\n",
    "        **pruning_params_2_by_4),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 2\n",
    "\n",
    "model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=0,\n",
    "    callbacks=tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    validation_split=0.1)\n",
    "\n",
    "_, pruned_model_accuracy = model.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Pruned test accuracy:', pruned_model_accuracy)\n",
    "\n",
    "model = tfmot.sparsity.keras.strip_pruning(model)\n",
    "\n",
    "import tempfile\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "_, tflite_file = tempfile.mkstemp('.tflite')\n",
    "print('Saved converted pruned model to:', tflite_file)\n",
    "with open(tflite_file, 'wb') as f:\n",
    "  f.write(tflite_model)\n",
    "\n",
    "  # Load tflite file with the created pruned model\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_file, experimental_preserve_all_tensors=True)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "details = interpreter.get_tensor_details()\n",
    "\n",
    "# Weights of the dense layer that has been pruned.\n",
    "tensor_name = 'structural_pruning_dense/MatMul'\n",
    "detail = [x for x in details if tensor_name in x[\"name\"]]\n",
    "\n",
    "# We need the first layer.\n",
    "tensor_data = interpreter.tensor(detail[0][\"index\"])()\n",
    "\n",
    "print(f\"Shape of Dense layer is {tensor_data.shape}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# The value 24 is chosen for convenience.\n",
    "width = height = 24\n",
    "\n",
    "subset_values_to_display = tensor_data[0:height, 0:width]\n",
    "\n",
    "val_ones = np.ones([height, width])\n",
    "val_zeros = np.zeros([height, width])\n",
    "subset_values_to_display = np.where(abs(subset_values_to_display) > 0, val_ones, val_zeros)\n",
    "\n",
    "\n",
    "def plot_separation_lines(height, width):\n",
    "\n",
    "    block_size = [1, 4]\n",
    "\n",
    "    # Add separation lines to the figure.\n",
    "    num_hlines = int((height - 1) / block_size[0])\n",
    "    num_vlines = int((width - 1) / block_size[1])\n",
    "    line_y_pos = [y * block_size[0] for y in range(1, num_hlines + 1)]\n",
    "    line_x_pos = [x * block_size[1] for x in range(1, num_vlines + 1)]\n",
    "\n",
    "    for y_pos in line_y_pos:\n",
    "        plt.plot([-0.5, width], [y_pos - 0.5 , y_pos - 0.5], color='w')\n",
    "\n",
    "    for x_pos in line_x_pos:\n",
    "        plt.plot([x_pos - 0.5, x_pos - 0.5], [-0.5, height], color='w')\n",
    "\n",
    "        plot_separation_lines(height, width)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(subset_values_to_display)\n",
    "plt.colorbar()\n",
    "plt.title(\"Structural pruning for Dense layer\")\n",
    "plt.show()\n",
    "\n",
    "# Get weights of the convolutional layer that has been pruned with 2 by 4 sparsity.\n",
    "op_details = interpreter._get_ops_details()\n",
    "op_name = 'CONV_2D'\n",
    "op_detail = [x for x in op_details if op_name in x[\"op_name\"]]\n",
    "tensor_data = interpreter.tensor(op_detail[1][\"inputs\"][1])()\n",
    "print(f\"Shape of the weight tensor is {tensor_data.shape}\")\n",
    "\n",
    "\n",
    "weights_to_display = tf.reshape(tensor_data, [tf.reduce_prod(tensor_data.shape[:-1]), -1])\n",
    "weights_to_display = weights_to_display[0:width, 0:height]\n",
    "\n",
    "val_ones = np.ones([height, width])\n",
    "val_zeros = np.zeros([height, width])\n",
    "subset_values_to_display = np.where(abs(weights_to_display) > 1e-9, val_ones, val_zeros)\n",
    "\n",
    "plot_separation_lines(height, width)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(subset_values_to_display)\n",
    "plt.colorbar()\n",
    "plt.title(\"Structurally pruned weights for Conv2D layer\")\n",
    "plt.show()\n",
    "\n",
    "# Get weights of the convolutional layer that has been pruned with random pruning.\n",
    "tensor_name = 'pruning_sparsity_0_5/Conv2D'\n",
    "detail = [x for x in details if tensor_name in x[\"name\"]]\n",
    "tensor_data = interpreter.tensor(detail[0][\"index\"])()\n",
    "print(f\"Shape of the weight tensor is {tensor_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GetPlot(model_results, features):\n",
    "        ymin = 1\n",
    "        ymax = 20000\n",
    "        layerX = [np.ones(len(features))]\n",
    "        layerY = [np.linspace(ymin,ymax,len(features))]\n",
    "        layer_probabilities = [np.ones(len(features))]\n",
    "        layer_weights = []\n",
    "        for i in range(len(model_results['activation_prob'])):\n",
    "                layer_probabilities.append(model_results['activation_prob'][i])\n",
    "                layer_weights.append(model_results['weights'][i])\n",
    "                layerX.append(np.ones(len(model_results['activation_prob'][i]))*(i+2))\n",
    "                if i == len(model_results['activation_prob'])-1:\n",
    "                        layerY.append(np.array([(ymax-ymin)/2]))\n",
    "                else:\n",
    "                        layerY.append(np.linspace(ymin,ymax,len(model_results['activation_prob'][i])))\n",
    "\n",
    "        line_iXY = []\n",
    "        line_fXY = []\n",
    "        line_W = []\n",
    "        node_XY = []\n",
    "        node_W = []\n",
    "\n",
    "        cutoff = 0.0\n",
    "        for iW, w in enumerate(layer_weights):\n",
    "                for iY, iprob in enumerate(layer_probabilities[iW]):\n",
    "                        if iprob > cutoff:\n",
    "                                node_XY.append([layerX[iW][iY],layerY[iW][iY]])\n",
    "                                node_W.append(layer_probabilities[iW][iY])\n",
    "                                for jY, jprob in enumerate(layer_probabilities[iW+1]):\n",
    "                                        if jprob > cutoff:\n",
    "                                                node_XY.append([layerX[iW+1][jY],layerY[iW+1][jY]])\n",
    "                                                node_W.append(layer_probabilities[iW+1][jY])\n",
    "                                                line_iXY.append([layerX[iW][iY],layerY[iW][iY]])\n",
    "                                                line_fXY.append([layerX[iW+1][jY],layerY[iW+1][jY]])\n",
    "                                                line_W.append(np.abs(w[iY,jY]/np.max(np.abs(w[:,jY]))))    \n",
    "        line_iXY = np.array(line_iXY)\n",
    "        line_fXY = np.array(line_fXY)\n",
    "        line_W = np.array(line_W)\n",
    "        node_XY = np.array(node_XY)\n",
    "        node_W = np.array(node_W)\n",
    "        fig = plt.figure(figsize=(12,6))\n",
    "        ax = fig.add_subplot(111)   \n",
    "        for A, B, W in zip(line_iXY, line_fXY, line_W):\n",
    "                ax.plot([A[0]+0.03, B[0]-0.03], [A[1], B[1]], c=cm.Blues(W), linewidth=0.5, alpha=0.5, linestyle='-', marker=' ', markersize=0)\n",
    "        ax.scatter(node_XY[:,0],node_XY[:,1],c=node_W,cmap=cm.Reds,s=30,alpha=1.0)\n",
    "        # plt.show()\n",
    "        \n",
    "        Plot = {}\n",
    "        Plot['iXY'] = line_iXY\n",
    "        Plot['fXY'] = line_fXY\n",
    "        Plot['lineW'] = line_W\n",
    "        Plot['nodeXY'] = node_XY\n",
    "        Plot['nodeW'] = node_W\n",
    "        \n",
    "        return Plot\n",
    "\n",
    "def PruneModel(model,pdf_file, features, target):\n",
    "\n",
    "\n",
    "        # probability cutoff\n",
    "        pdf =  pd.read_hdf(pdf_file, key='df').sample(frac=0.5).copy()\n",
    "        cutoff = 0.0\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        model.fit(pdf[features].values, pdf[target].values, epochs=100, batch_size=128, verbose=0, validation_split=0.2, callbacks=[es])\n",
    "        layer_in_model = []\n",
    "        for i in range(len(model.layers)):\n",
    "                layer_in_model.append(model.get_layer(index=i))\n",
    "\n",
    "        X = pdf[features].values \n",
    "        layer_activation_probabilities = {}\n",
    "        layer_weights = {}\n",
    "        layer_biases = {}\n",
    "        weights_for_plot = []\n",
    "        for i in range(len(layer_in_model)):\n",
    "                \n",
    "                layer_outputs = layer_in_model[i](X)\n",
    "                layer_activation_probabilities[i] = np.count_nonzero(np.array(layer_outputs), axis=0)/X.shape[0]\n",
    "                layer_weights[i] = layer_in_model[i].get_weights()[0]\n",
    "                layer_biases[i] = layer_in_model[i].get_weights()[1]\n",
    "                weights_for_plot.append(layer_weights[i])\n",
    "                for j in range(layer_activation_probabilities[i].shape[0]):\n",
    "                        if layer_activation_probabilities[i][j] <= cutoff:\n",
    "                                        layer_weights[i][:,j] = 0.0\n",
    "                                        layer_biases[i][j] = 0.0\n",
    "                X = layer_in_model[i](X)   \n",
    "                \n",
    "        for i in range(1,len(layer_in_model)):\n",
    "                for j in range(layer_activation_probabilities[i-1].shape[0]):\n",
    "                        if layer_activation_probabilities[i-1][j] <= cutoff:\n",
    "                                        layer_weights[i][j,:] = 0.0                               \n",
    "\n",
    "        new_weights = {}\n",
    "        new_biases = {}\n",
    "        for i in range(1,len(layer_in_model)):\n",
    "                weights = []\n",
    "                bias = []\n",
    "                for j in range(layer_weights[i].shape[0]):\n",
    "                        w = layer_weights[i][j,:]\n",
    "                        w = w[w != 0.0]\n",
    "                        if w.size != 0:\n",
    "                                weights.append(w)\n",
    "                for j in range(layer_biases[i].shape[0]):\n",
    "                        b = layer_biases[i][j]\n",
    "                        if b != 0.0:\n",
    "                                bias.append(b)\n",
    "                                \n",
    "                new_weights[i] = np.array(weights)\n",
    "                new_biases[i] = np.array(bias)\n",
    "                \n",
    "        active_nodes = []\n",
    "        for i in range(len(layer_in_model)):\n",
    "                if np.count_nonzero(layer_activation_probabilities[i])  == 0:\n",
    "                        active_nodes.append(1)\n",
    "                else: \n",
    "                        active_nodes.append(np.count_nonzero(layer_activation_probabilities[i]))\n",
    "\n",
    "        weights_for_new_model = []\n",
    "        bias_for_new_model = []\n",
    "        for w in new_weights:\n",
    "                weights_for_new_model.append(new_weights[w])\n",
    "                bias_for_new_model.append(new_biases[w])\n",
    "\n",
    "        weights_for_plot = []\n",
    "        for w in layer_weights:\n",
    "                weights_for_plot.append(layer_weights[w])\n",
    "\n",
    "        nodes_for_new_model = active_nodes \n",
    "        layer_activation_probabilities_for_plot = []\n",
    "        for i in range(len(layer_in_model)):\n",
    "                layer_activation_probabilities_for_plot.append(layer_activation_probabilities[i])\n",
    "                \n",
    "        mse_of_model = model.evaluate(pdf[features], pdf[target], verbose=0)[1]\n",
    "        print('MSE of model: ', mse_of_model)   \n",
    "\n",
    "        reduced_sum = 0\n",
    "        for i in range(1,len(layer_in_model)):\n",
    "                w2 = tf.reduce_sum(tf.square(layer_weights[i]))\n",
    "                reduced_sum += w2.numpy()\n",
    "        print('Reduced sum of W^2: ', reduced_sum)\n",
    "\n",
    "\n",
    "        model_results = {}\n",
    "        model_results['mse'] = mse_of_model\n",
    "        model_results['activation_prob'] = layer_activation_probabilities_for_plot\n",
    "        model_results['weights'] = weights_for_plot\n",
    "        model_results['reduced_sum'] = reduced_sum\n",
    "        \n",
    "        new_model = {}\n",
    "        new_model['weights'] = weights_for_new_model\n",
    "        new_model['bias'] = bias_for_new_model\n",
    "        new_model['nodes'] = nodes_for_new_model\n",
    "        \n",
    "        return model_results, new_model\n",
    "\n",
    "def GetDNNPlot(kernel_reg = False, lam=0.0):\n",
    "                \n",
    "        filename = \"/lustre/isaac/scratch/tmengel/jet-background-subtraction/datasets/root-files/AuAu_R04/test/AuAu_R04_50_test_sample.h5\"\n",
    "        dnn_features = [\"jet_pt_raw\",\"jet_nparts\",\"jet_area\",\"jet_angularity\",\"jet_track_pt_0\",\n",
    "                                \"jet_track_pt_1\",\"jet_track_pt_2\",\"jet_track_pt_3\",\"jet_track_pt_4\",\n",
    "                                \"jet_track_pt_5\",\"jet_track_pt_6\",\"jet_track_pt_7\"]\n",
    "        \n",
    "        # dnn_features = [\"jet_pt_raw\",\"jet_nparts\",\"jet_area\",\"jet_angularity\",\"jet_track_pt_0\"]\n",
    "        \n",
    "\n",
    "        target='jet_pt_truth'\n",
    "                        \n",
    "        if kernel_reg:           \n",
    "                model = keras.Sequential([\n",
    "                tf.keras.layers.Dense(100,input_shape=(len(dnn_features),),activation='relu',kernel_initializer='he_uniform',kernel_regularizer=keras.regularizers.l2(lam)),\n",
    "                tf.keras.layers.Dense(100,activation='relu',kernel_initializer='he_uniform',kernel_regularizer=keras.regularizers.l2(lam)),\n",
    "                tf.keras.layers.Dense(50,activation='relu',kernel_initializer='he_uniform',kernel_regularizer=keras.regularizers.l2(lam)),\n",
    "                tf.keras.layers.Dense(1,activation='relu',kernel_initializer='he_uniform',kernel_regularizer=keras.regularizers.l2(lam))\n",
    "                ]\n",
    "                )\n",
    "        else:\n",
    "                model = keras.Sequential([\n",
    "                tf.keras.layers.Dense(100,input_shape=(len(dnn_features),),activation='relu',kernel_initializer='he_uniform'),\n",
    "                tf.keras.layers.Dense(100,activation='relu',kernel_initializer='he_uniform'),\n",
    "                tf.keras.layers.Dense(50,activation='relu',kernel_initializer='he_uniform'),\n",
    "                tf.keras.layers.Dense(1,activation='relu',kernel_initializer='he_uniform')\n",
    "                ])\n",
    "                \n",
    "                \n",
    "        model_results, new_model = PruneModel(model,filename,features=dnn_features,target=target)\n",
    "        PLOTS = GetPlot(model_results, dnn_features)\n",
    "        trainable = 0\n",
    "        new_nodes = new_model['nodes']\n",
    "        print(new_nodes)\n",
    "        new_nodes = np.array(new_nodes)\n",
    "        trainable = np.sum(new_nodes)\n",
    "        PLOTS['trainable'] = trainable\n",
    "        PLOTS['new_nodes'] = new_nodes\n",
    "        PLOTS['mse'] = model_results['mse']\n",
    "        PLOTS['lam'] = lam\n",
    "        PLOTS['reduced_sum'] = model_results['reduced_sum']\n",
    "        \n",
    "        return PLOTS\n",
    "\n",
    "def GetNNPlot(architechture, lam=0.0):\n",
    "        \n",
    "        filename = \"/lustre/isaac/scratch/tmengel/jet-background-subtraction/datasets/root-files/AuAu_R04/test/AuAu_R04_50_test_sample.h5\"\n",
    "        # dnn_features = [\"jet_pt_raw\",\"jet_nparts\",\"jet_area\",\"jet_angularity\",\"jet_track_pt_0\",\n",
    "        #                         \"jet_track_pt_1\",\"jet_track_pt_2\",\"jet_track_pt_3\",\"jet_track_pt_4\",\n",
    "        #                         \"jet_track_pt_5\",\"jet_track_pt_6\",\"jet_track_pt_7\"]\n",
    "        \n",
    "        dnn_features = [\"jet_pt_raw\",\"jet_nparts\",\"jet_area\",\"jet_angularity\",\"jet_track_pt_0\"]\n",
    "\n",
    "        target='jet_pt_truth'\n",
    "        layers_list = []\n",
    "        # input layer    \n",
    "        layers_list.append(layers.Dense(architechture[0],input_shape=(len(dnn_features),),activation='relu',kernel_initializer='he_uniform',kernel_regularizer=keras.regularizers.l2(lam)))\n",
    "        # hidden layersl\n",
    "        for i in range(1,len(architechture)-1):\n",
    "                layers_list.append(layers.Dense(architechture[i],activation='relu',kernel_initializer='he_uniform',kernel_regularizer=keras.regularizers.l2(lam)))\n",
    "        # output layer\n",
    "        layers_list.append(layers.Dense(architechture[-1],activation='relu',kernel_initializer='he_uniform',kernel_regularizer=keras.regularizers.l2(lam)))\n",
    "\n",
    "        model = keras.Sequential(layers_list)\n",
    "\n",
    "        model_results, new_model = PruneModel(model,filename,features=dnn_features,target=target)\n",
    "        PLOTS = GetPlot(model_results, dnn_features)\n",
    "        trainable = 0\n",
    "        new_nodes = new_model['nodes']\n",
    "        # print(new_nodes)\n",
    "        new_nodes = np.array(new_nodes)\n",
    "        trainable = np.sum(new_nodes)\n",
    "        PLOTS['trainable'] = trainable\n",
    "        PLOTS['new_nodes'] = new_nodes\n",
    "        PLOTS['mse'] = model_results['mse']\n",
    "        PLOTS['lam'] = lam\n",
    "        PLOTS['reduced_sum'] = model_results['reduced_sum']\n",
    "        \n",
    "        return PLOTS\n",
    "\n",
    "def CombineLayersBelowNodeThreshold(node_list, threshold):\n",
    "        # combine layers below a certain threshold to the previous layer\n",
    "        new_node_list = []\n",
    "        # loop in reverse order\n",
    "        for i in range(len(node_list)-2,-1,-1):\n",
    "                if node_list[i] < threshold:\n",
    "                        node_list[i-1] = node_list[i-1] + node_list[i]\n",
    "                        node_list[i] = 0\n",
    "        for i in range(len(node_list)):\n",
    "                if node_list[i] != 0:\n",
    "                        new_node_list.append(node_list[i])\n",
    "        return new_node_list\n",
    "\n",
    "\n",
    "def MinimizeArchitecture(mse_threshold=0.05):\n",
    "        NETWORKS = {}\n",
    "        NETWORKS['initial'] = GetDNNPlot(kernel_reg=True, lam=0.01)\n",
    "        \n",
    "        original_mse = NETWORKS['initial']['mse']\n",
    "        reduced_sum = NETWORKS['initial']['reduced_sum']\n",
    "        new_nodes = NETWORKS['initial']['new_nodes']\n",
    "        new_nodes_trimmed = CombineLayersBelowNodeThreshold(new_nodes, 3)\n",
    "\n",
    "        mse = original_mse\n",
    "        prunning_iteration = 0\n",
    "        while True:\n",
    "                percent_change = (mse - original_mse)/original_mse\n",
    "                lam = (mse_threshold*original_mse)/reduced_sum\n",
    "                new_nodes_trimmed = CombineLayersBelowNodeThreshold(new_nodes, 3)\n",
    "                \n",
    "                if len(new_nodes_trimmed) == 1:\n",
    "                        print('Only one layer left')\n",
    "                        break\n",
    "                \n",
    "                print(f'Prunning iteration: {prunning_iteration}')\n",
    "                print(f'MSE: {mse}, Original MSE: {original_mse}, Percent Change: {percent_change*100}%')\n",
    "               \n",
    "                if percent_change > mse_threshold:\n",
    "                        print('Percent change is greater than threshold')\n",
    "                        break\n",
    "                \n",
    "                else:\n",
    "                        print(f'Lambda: {lam}')\n",
    "                        print(f'New Nodes: {new_nodes}')\n",
    "                        print(f'New Nodes Trimmed: {new_nodes_trimmed}') \n",
    "                                               \n",
    "                        TEMP_NETWORK = GetNNPlot(new_nodes_trimmed, lam)\n",
    "                        mse_tmp = TEMP_NETWORK['mse']\n",
    "                        if mse_tmp > 100: # error try again\n",
    "                                continue\n",
    "                        prunning_iteration += 1 \n",
    "                        NETWORKS[f'iter_{prunning_iteration}'] = TEMP_NETWORK\n",
    "                        mse = NETWORKS[f'iter_{prunning_iteration}']['mse']\n",
    "                        new_nodes = NETWORKS[f'iter_{prunning_iteration}']['new_nodes']\n",
    "                        reduced_sum = NETWORKS[f'iter_{prunning_iteration}']['reduced_sum']\n",
    "\n",
    "        return NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMEPOCHS = 100\n",
    "PHASEMAX = 100\n",
    "PERCENTPILEUP = 0.5\n",
    "NUMTRAINING = 20000\n",
    "ModelOutputName = 'tanner_test'\n",
    "AUGMENTATION = 8\n",
    "TRACELENGTH = 250\n",
    "INPUTSIZE = TRACELENGTH-4*AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PileupNet(inputSize, \n",
    "                kernel_size = 4,\n",
    "                filters = 16,\n",
    "                dense_units = 32,\n",
    "                verbose=True,\n",
    "                kernel_reg = False,\n",
    "                lambda_reg = 0.000\n",
    "                ):\n",
    "\n",
    "\n",
    "    if kernel_reg:\n",
    "         model = keras.Sequential(\n",
    "              [\n",
    "                    tf.keras.layers.Conv1D(kernel_size=kernel_size, filters=filters, activation='relu', input_shape=(inputSize,1), kernel_initializer='he_uniform', kernel_regularizer=tf.keras.regularizers.l2(lambda_reg), name='conv1'),\n",
    "                    tf.keras.layers.Flatten(name='flatten'),\n",
    "                    tf.keras.layers.Dense(dense_units, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=tf.keras.regularizers.l2(lambda_reg), name='dense'),\n",
    "                    tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer='he_uniform', kernel_regularizer=tf.keras.regularizers.l2(lambda_reg), name='output')\n",
    "                ],\n",
    "                name='PileupNet'\n",
    "            )\n",
    "    else:\n",
    "        model = keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Conv1D(kernel_size=kernel_size, filters=filters, activation='relu', input_shape=(inputSize,1), kernel_initializer='he_uniform', name='conv1'),\n",
    "                tf.keras.layers.Flatten(name='flatten'),\n",
    "                tf.keras.layers.Dense(dense_units, activation='relu', kernel_initializer='he_uniform', name='dense'),\n",
    "                tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer='he_uniform', name='output')\n",
    "            ],\n",
    "            name='PileupNet'\n",
    "        )\n",
    "\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "def AmpNet(inputSize, \n",
    "                kernel_size = 10,\n",
    "                filters = 16,\n",
    "                dense_units = 128,\n",
    "                verbose=True,\n",
    "                kernel_reg = False,\n",
    "                lambda_reg = 0.000\n",
    "                ):\n",
    "\n",
    "\n",
    "    if kernel_reg:\n",
    "         model = keras.Sequential(\n",
    "              [\n",
    "                    tf.keras.layers.Conv1D(kernel_size=kernel_size, filters=filters, activation='tanh', input_shape=(inputSize,1), kernel_initializer='he_uniform', kernel_regularizer=tf.keras.regularizers.l2(lambda_reg), name='conv1'),\n",
    "                    tf.keras.layers.Flatten(name='flatten'),\n",
    "                    tf.keras.layers.Dense(dense_units, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=tf.keras.regularizers.l2(lambda_reg), name='dense'),\n",
    "                    tf.keras.layers.Dense(1, activation='linear', kernel_initializer='he_uniform', kernel_regularizer=tf.keras.regularizers.l2(lambda_reg), name='output')\n",
    "                ],\n",
    "                name='AmpNet'\n",
    "            )\n",
    "    else:\n",
    "        model = keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Conv1D(kernel_size=kernel_size, filters=filters, activation='tanh', input_shape=(inputSize,1), kernel_initializer='he_uniform', name='conv1'),\n",
    "                tf.keras.layers.Flatten(name='flatten'),\n",
    "                tf.keras.layers.Dense(dense_units, activation='relu', kernel_initializer='he_uniform', name='dense'),\n",
    "                tf.keras.layers.Dense(1, activation='linear', kernel_initializer='he_uniform', name='output')\n",
    "            ],\n",
    "            name='AmpNet'\n",
    "        )\n",
    "\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "def PhaseNet(inputSize, \n",
    "                kernel_size = 10,\n",
    "                filters = 16,\n",
    "                dense_units = 128,\n",
    "                verbose=True,\n",
    "                kernel_reg = False,\n",
    "                lambda_reg = 0.000\n",
    "            ):\n",
    "\n",
    "\n",
    "    if kernel_reg:\n",
    "         model = keras.Sequential(\n",
    "              [\n",
    "                    tf.keras.layers.Conv1D(kernel_size=kernel_size, filters=filters, activation='relu', input_shape=(inputSize,1), kernel_initializer='he_uniform', kernel_regularizer=tf.keras.regularizers.l2(lambda_reg), name='conv1'),\n",
    "                    tf.keras.layers.Flatten(name='flatten'),\n",
    "                    tf.keras.layers.Dense(dense_units, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=tf.keras.regularizers.l2(lambda_reg), name='dense'),\n",
    "                    tf.keras.layers.Dense(1, activation='linear', kernel_initializer='he_uniform', kernel_regularizer=tf.keras.regularizers.l2(lambda_reg), name='output')\n",
    "                ],\n",
    "                name='PhaseNet'\n",
    "            )\n",
    "    else:\n",
    "        model = keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Conv1D(kernel_size=kernel_size, filters=filters, activation='relu', input_shape=(inputSize,1), kernel_initializer='he_uniform', name='conv1'),\n",
    "                tf.keras.layers.Flatten(name='flatten'),\n",
    "                tf.keras.layers.Dense(dense_units, activation='relu', kernel_initializer='he_uniform', name='dense'),\n",
    "                tf.keras.layers.Dense(1, activation='linear', kernel_initializer='he_uniform', name='output')\n",
    "            ],\n",
    "            name='PhaseNet'\n",
    "        )\n",
    "\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  430532\n",
      "Test size:  107633\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 218, 1)]          0         \n",
      "                                                                 \n",
      " conv1 (Conv1D)              (None, 215, 16)           80        \n",
      "                                                                 \n",
      " flatten1 (Flatten)          (None, 3440)              0         \n",
      "                                                                 \n",
      " dense1 (Dense)              (None, 32)                110112    \n",
      "                                                                 \n",
      " pileupoutput (Dense)        (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 110,225\n",
      "Trainable params: 110,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "3343/3364 [============================>.] - ETA: 0s - loss: 67.4973 - accuracy: 0.1622\n",
      "Epoch 1: val_loss improved from inf to 34.69427, saving model to tanner_test.h5\n",
      "3364/3364 [==============================] - 14s 4ms/step - loss: 67.2660 - accuracy: 0.1626 - val_loss: 34.6943 - val_accuracy: 0.2147\n",
      "Epoch 2/100\n",
      "3359/3364 [============================>.] - ETA: 0s - loss: 31.8684 - accuracy: 0.2143\n",
      "Epoch 2: val_loss improved from 34.69427 to 30.49607, saving model to tanner_test.h5\n",
      "3364/3364 [==============================] - 14s 4ms/step - loss: 31.8599 - accuracy: 0.2143 - val_loss: 30.4961 - val_accuracy: 0.1310\n",
      "Epoch 3/100\n",
      "3341/3364 [============================>.] - ETA: 0s - loss: 23.9133 - accuracy: 0.2223\n",
      "Epoch 3: val_loss improved from 30.49607 to 20.89341, saving model to tanner_test.h5\n",
      "3364/3364 [==============================] - 13s 4ms/step - loss: 23.9261 - accuracy: 0.2220 - val_loss: 20.8934 - val_accuracy: 0.2882\n",
      "Epoch 4/100\n",
      "3350/3364 [============================>.] - ETA: 0s - loss: 17.0588 - accuracy: 0.2660\n",
      "Epoch 4: val_loss improved from 20.89341 to 13.75601, saving model to tanner_test.h5\n",
      "3364/3364 [==============================] - 13s 4ms/step - loss: 17.0629 - accuracy: 0.2660 - val_loss: 13.7560 - val_accuracy: 0.2100\n",
      "Epoch 5/100\n",
      "3359/3364 [============================>.] - ETA: 0s - loss: 12.2091 - accuracy: 0.2804\n",
      "Epoch 5: val_loss improved from 13.75601 to 12.45408, saving model to tanner_test.h5\n",
      "3364/3364 [==============================] - 13s 4ms/step - loss: 12.2070 - accuracy: 0.2804 - val_loss: 12.4541 - val_accuracy: 0.2962\n",
      "Epoch 6/100\n",
      "3364/3364 [==============================] - ETA: 0s - loss: 9.9816 - accuracy: 0.2972\n",
      "Epoch 6: val_loss improved from 12.45408 to 9.27584, saving model to tanner_test.h5\n",
      "3364/3364 [==============================] - 13s 4ms/step - loss: 9.9816 - accuracy: 0.2972 - val_loss: 9.2758 - val_accuracy: 0.2653\n",
      "Epoch 7/100\n",
      "3343/3364 [============================>.] - ETA: 0s - loss: 8.6832 - accuracy: 0.3110\n",
      "Epoch 7: val_loss improved from 9.27584 to 7.96297, saving model to tanner_test.h5\n",
      "3364/3364 [==============================] - 13s 4ms/step - loss: 8.6751 - accuracy: 0.3108 - val_loss: 7.9630 - val_accuracy: 0.3151\n",
      "Epoch 8/100\n",
      "3353/3364 [============================>.] - ETA: 0s - loss: 7.7999 - accuracy: 0.3220\n",
      "Epoch 8: val_loss improved from 7.96297 to 7.26281, saving model to tanner_test.h5\n",
      "3364/3364 [==============================] - 13s 4ms/step - loss: 7.7954 - accuracy: 0.3220 - val_loss: 7.2628 - val_accuracy: 0.3071\n",
      "Epoch 9/100\n",
      "3361/3364 [============================>.] - ETA: 0s - loss: 6.9553 - accuracy: 0.3375\n",
      "Epoch 9: val_loss did not improve from 7.26281\n",
      "3364/3364 [==============================] - 13s 4ms/step - loss: 6.9537 - accuracy: 0.3375 - val_loss: 7.2767 - val_accuracy: 0.3261\n",
      "Epoch 10/100\n",
      "3350/3364 [============================>.] - ETA: 0s - loss: 6.3736 - accuracy: 0.3448\n",
      "Epoch 10: val_loss improved from 7.26281 to 7.09542, saving model to tanner_test.h5\n",
      "3364/3364 [==============================] - 13s 4ms/step - loss: 6.4159 - accuracy: 0.3448 - val_loss: 7.0954 - val_accuracy: 0.2743\n",
      "Epoch 11/100\n",
      "3351/3364 [============================>.] - ETA: 0s - loss: 6.0473 - accuracy: 0.3508\n",
      "Epoch 11: val_loss did not improve from 7.09542\n",
      "3364/3364 [==============================] - 13s 4ms/step - loss: 6.0464 - accuracy: 0.3507 - val_loss: 7.4527 - val_accuracy: 0.2416\n",
      "Epoch 12/100\n",
      "3349/3364 [============================>.] - ETA: 0s - loss: 5.7441 - accuracy: 0.3561\n",
      "Epoch 12: val_loss improved from 7.09542 to 5.91273, saving model to tanner_test.h5\n",
      "3364/3364 [==============================] - 13s 4ms/step - loss: 5.7386 - accuracy: 0.3559 - val_loss: 5.9127 - val_accuracy: 0.3555\n",
      "Epoch 13/100\n",
      "3356/3364 [============================>.] - ETA: 0s - loss: 5.4808 - accuracy: 0.3595\n",
      "Epoch 13: val_loss improved from 5.91273 to 5.28985, saving model to tanner_test.h5\n",
      "3364/3364 [==============================] - 13s 4ms/step - loss: 5.4768 - accuracy: 0.3596 - val_loss: 5.2899 - val_accuracy: 0.3746\n",
      "Epoch 14/100\n",
      "3358/3364 [============================>.] - ETA: 0s - loss: 5.2080 - accuracy: 0.3638\n",
      "Epoch 14: val_loss did not improve from 5.28985\n",
      "3364/3364 [==============================] - 13s 4ms/step - loss: 5.2077 - accuracy: 0.3637 - val_loss: 6.0667 - val_accuracy: 0.4075\n",
      "Epoch 15/100\n",
      "3362/3364 [============================>.] - ETA: 0s - loss: 5.0179 - accuracy: 0.3662\n",
      "Epoch 15: val_loss improved from 5.28985 to 4.93209, saving model to tanner_test.h5\n",
      "3364/3364 [==============================] - 14s 4ms/step - loss: 5.0173 - accuracy: 0.3662 - val_loss: 4.9321 - val_accuracy: 0.3583\n",
      "Epoch 16/100\n",
      "3357/3364 [============================>.] - ETA: 0s - loss: 4.8099 - accuracy: 0.3673\n",
      "Epoch 16: val_loss improved from 4.93209 to 4.83190, saving model to tanner_test.h5\n",
      "3364/3364 [==============================] - 13s 4ms/step - loss: 4.8064 - accuracy: 0.3674 - val_loss: 4.8319 - val_accuracy: 0.3886\n",
      "Epoch 17/100\n",
      "3360/3364 [============================>.] - ETA: 0s - loss: 4.6294 - accuracy: 0.3693\n",
      "Epoch 17: val_loss did not improve from 4.83190\n",
      "3364/3364 [==============================] - 13s 4ms/step - loss: 4.6275 - accuracy: 0.3692 - val_loss: 5.2960 - val_accuracy: 0.3996\n",
      "Epoch 18/100\n",
      "3361/3364 [============================>.] - ETA: 0s - loss: 4.4907 - accuracy: 0.3744\n",
      "Epoch 18: val_loss did not improve from 4.83190\n",
      "3364/3364 [==============================] - 14s 4ms/step - loss: 4.4890 - accuracy: 0.3744 - val_loss: 5.7904 - val_accuracy: 0.3467\n",
      "Epoch 19/100\n",
      "3358/3364 [============================>.] - ETA: 0s - loss: 4.3624 - accuracy: 0.3777\n",
      "Epoch 19: val_loss did not improve from 4.83190\n",
      "3364/3364 [==============================] - 14s 4ms/step - loss: 4.3605 - accuracy: 0.3776 - val_loss: 5.2432 - val_accuracy: 0.4043\n",
      "Epoch 20/100\n",
      "3361/3364 [============================>.] - ETA: 0s - loss: 4.1895 - accuracy: 0.3806\n",
      "Epoch 20: val_loss improved from 4.83190 to 4.81148, saving model to tanner_test.h5\n",
      "3364/3364 [==============================] - 14s 4ms/step - loss: 4.1899 - accuracy: 0.3806 - val_loss: 4.8115 - val_accuracy: 0.4153\n",
      "Epoch 21/100\n",
      "3361/3364 [============================>.] - ETA: 0s - loss: 4.1036 - accuracy: 0.3833\n",
      "Epoch 21: val_loss improved from 4.81148 to 4.33098, saving model to tanner_test.h5\n",
      "3364/3364 [==============================] - 13s 4ms/step - loss: 4.1115 - accuracy: 0.3834 - val_loss: 4.3310 - val_accuracy: 0.4058\n",
      "Epoch 22/100\n",
      "3359/3364 [============================>.] - ETA: 0s - loss: 4.0290 - accuracy: 0.3853\n",
      "Epoch 22: val_loss improved from 4.33098 to 4.22737, saving model to tanner_test.h5\n",
      "3364/3364 [==============================] - 13s 4ms/step - loss: 4.0277 - accuracy: 0.3854 - val_loss: 4.2274 - val_accuracy: 0.4123\n",
      "Epoch 23/100\n",
      "3354/3364 [============================>.] - ETA: 0s - loss: 3.9354 - accuracy: 0.3881\n",
      "Epoch 23: val_loss improved from 4.22737 to 4.21513, saving model to tanner_test.h5\n",
      "3364/3364 [==============================] - 13s 4ms/step - loss: 3.9325 - accuracy: 0.3882 - val_loss: 4.2151 - val_accuracy: 0.4095\n",
      "Epoch 24/100\n",
      "3362/3364 [============================>.] - ETA: 0s - loss: 3.8700 - accuracy: 0.3904\n",
      "Epoch 24: val_loss did not improve from 4.21513\n",
      "3364/3364 [==============================] - 13s 4ms/step - loss: 3.8704 - accuracy: 0.3904 - val_loss: 4.3473 - val_accuracy: 0.4035\n",
      "Epoch 25/100\n",
      "3357/3364 [============================>.] - ETA: 0s - loss: 3.7571 - accuracy: 0.3911\n",
      "Epoch 25: val_loss improved from 4.21513 to 4.00371, saving model to tanner_test.h5\n",
      "3364/3364 [==============================] - 14s 4ms/step - loss: 3.7555 - accuracy: 0.3911 - val_loss: 4.0037 - val_accuracy: 0.3524\n",
      "Epoch 26/100\n",
      "3356/3364 [============================>.] - ETA: 0s - loss: 3.6919 - accuracy: 0.3926"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2607530/2800144372.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModelOutputName\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpileup_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUMEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/myhepenv/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myhepenv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1615\u001b[0m                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m                         \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1617\u001b[0;31m                         \u001b[0m_use_cached_eval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1618\u001b[0m                     )\n\u001b[1;32m   1619\u001b[0m                     val_logs = {\n",
      "\u001b[0;32m~/.conda/envs/myhepenv/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myhepenv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1940\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1941\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1942\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1943\u001b[0m                         with tf.profiler.experimental.Trace(\n\u001b[1;32m   1944\u001b[0m                             \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myhepenv/lib/python3.7/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36msteps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m             \u001b[0moriginal_spe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m             can_run_full_execution = (\n\u001b[1;32m   1376\u001b[0m                 \u001b[0moriginal_spe\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myhepenv/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    635\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m     raise NotImplementedError(\n\u001b[1;32m    639\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001b[0;32m~/.conda/envs/myhepenv/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \"\"\"\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m       \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m     \u001b[0;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m     \u001b[0;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myhepenv/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_read_variable_op\u001b[0;34m(self, no_copy)\u001b[0m\n\u001b[1;32m    687\u001b[0m       \u001b[0mThe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \"\"\"\n\u001b[0;32m--> 689\u001b[0;31m     \u001b[0mvariable_accessed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myhepenv/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mvariable_accessed\u001b[0;34m(variable)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_accessed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myhepenv/lib/python3.7/site-packages/tensorflow/python/eager/tape.py\u001b[0m in \u001b[0;36mvariable_accessed\u001b[0;34m(variable)\u001b[0m\n\u001b[1;32m    111\u001b[0m       distribution_strategy_context.get_strategy_and_replica_context())\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_container\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_local_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myhepenv/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mextended\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    844\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_use_with_coordinator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mextended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;34m\"\"\"`tf.distribute.StrategyExtended` with additional methods.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trace_train, trace_test, pileup_train, pileup_test = GetDataSet(type=\"phase\")\n",
    "model = TraceNet()\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# # early stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "\n",
    "# # model checkpoint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "mc = ModelCheckpoint(ModelOutputName+'.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(trace_train, pileup_train, epochs=NUMEPOCHS, batch_size=64, validation_split=0.5, callbacks=[es,mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
